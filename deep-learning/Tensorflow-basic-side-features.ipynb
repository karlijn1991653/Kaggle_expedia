{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow features \n",
    "\n",
    "https://www.tensorflow.org/recommenders/examples/featurization\n",
    "\n",
    "https://www.tensorflow.org/recommenders/examples/context_features\n",
    "\n",
    "This caveat aside, real-world models may well benefit from other time-based features such as time of day or day of the week, especially if the data has strong seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The handle \"movie_lens\" for the MovieLens dataset is deprecated. Prefer using \"movielens\" instead.\n",
      "WARNING:absl:The handle \"movie_lens\" for the MovieLens dataset is deprecated. Prefer using \"movielens\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'timestamp': 879024327,\n",
      " 'user_id': b'138'}\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movie_lens/100k-movies\", split=\"train\")\n",
    "\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item embeddings\n",
    "\n",
    "User and item embeddings: Initially, these embeddings will take on random values - but during training, we will adjust them so that embeddings of users and the movies they watch end up closer together.\n",
    "\n",
    "Taking raw categorical features and turning them into embeddings is normally a two-step process:\n",
    "\n",
    "- Firstly, we need to translate the raw values into a range of contiguous integers, normally by building a mapping (called a \"vocabulary\") that maps raw values (\"Star Wars\") to integers (say, 15).\n",
    "- Secondly, we need to take these integers and turn them into embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "movie_title_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_lookup.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 2, 59])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_lookup([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a large number of bins to reduce the chance of hash collisions.\n",
    "num_hashing_bins = 200_000\n",
    "\n",
    "movie_title_hashing = tf.keras.layers.experimental.preprocessing.Hashing(\n",
    "    num_bins=num_hashing_bins\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([101016,  96565])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_hashing([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "An embedding layer has two dimensions: the first dimension tells us how many distinct categories we can embed; the second tells us how large the vector representing each of them can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['Star Wars (1977)']\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['Star Wars (1977)']\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[ 0.00396277,  0.01172978, -0.02384911, -0.04837387, -0.04741423,\n",
       "         0.01612918,  0.0479348 ,  0.02198057, -0.02069057, -0.02909831,\n",
       "         0.03733841, -0.03273455, -0.01077179,  0.01432376,  0.02987571,\n",
       "        -0.01175674, -0.01337911,  0.02950252,  0.03352079, -0.00063802,\n",
       "         0.04012284, -0.02083405, -0.02146355,  0.03655815,  0.01027417,\n",
       "         0.01376143,  0.02866649,  0.0131222 , -0.04579788, -0.02678558,\n",
       "        -0.03854306, -0.0214591 ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the hashing approach.\n",
    "    input_dim=movie_title_lookup.vocab_size(),\n",
    "    output_dim=32\n",
    ")\n",
    "movie_title_model = tf.keras.Sequential([movie_title_lookup, movie_title_embedding])\n",
    "movie_title_model([\"Star Wars (1977)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the hashing approach.\n",
    "    input_dim=user_id_lookup.vocab_size(),\n",
    "    output_dim=32\n",
    ")\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '405']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_lookup.get_vocabulary()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'str'> input: 405\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'str'> input: 405\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.03022064, -0.00098227, -0.0162935 ,  0.02301618,  0.01949287,\n",
       "       -0.04085468, -0.01580148,  0.0021788 , -0.00351913, -0.02449911,\n",
       "       -0.00816481, -0.0109116 , -0.016825  , -0.0100292 , -0.01753429,\n",
       "       -0.0463519 ,  0.02666036,  0.00514909,  0.01995898,  0.02405507,\n",
       "        0.0125909 ,  0.04897085, -0.02830227,  0.04055488, -0.02390121,\n",
       "       -0.01923902,  0.04656236,  0.04607103, -0.02452377, -0.02794539,\n",
       "        0.00302502,  0.0306075 ], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_model('405')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 879024327.\n",
      "Timestamp: 875654590.\n",
      "Timestamp: 882075110.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Timestamp: {x['timestamp']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized timestamp: [[-0.84293705]].\n",
      "Normalized timestamp: [[-1.47352]].\n",
      "Normalized timestamp: [[-0.27203262]].\n"
     ]
    }
   ],
   "source": [
    "timestamp_normalization = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "timestamp_normalization.adapt(ratings.map(lambda x: x[\"timestamp\"]).batch(1024))\n",
    "\n",
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Normalized timestamp: {timestamp_normalization(x['timestamp'])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [8.74724710e+08 8.74743291e+08 8.74761871e+08]\n"
     ]
    }
   ],
   "source": [
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x145bf0ef0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x145bf0ef0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x145bf0ef0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Timestamp embedding: [[ 0.04888289  0.00913225 -0.00306313 -0.01845644 -0.00260611  0.03414357\n",
      "   0.02560953 -0.00913762  0.01979966 -0.04191906  0.04516757 -0.00173339\n",
      "  -0.03848343  0.04710786  0.04880596 -0.02773777 -0.03101053  0.03004799\n",
      "   0.02313245  0.04730508 -0.01537329 -0.01770693  0.04392416  0.00200449\n",
      "   0.01659519 -0.02663034  0.01308459  0.02205885 -0.03799649 -0.01079416\n",
      "  -0.03766627  0.00561325]].\n"
     ]
    }
   ],
   "source": [
    "timestamp_embedding_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "  tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32)\n",
    "])\n",
    "\n",
    "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
    "  print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}.\")                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.experimental.preprocessing.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x145d0b9e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x145d0b9e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x145d0b9e0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[b\"One Flew Over the Cuckoo's Nest (1975)\"]\n",
      "[[ 32 266 162   2 267 265  53]]\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
    "    print(row.numpy())\n",
    "    print(title_text(row).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', '1998', '1977', '1971', 'monty']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_text.get_vocabulary()[40:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential([\n",
    "                  tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "                  tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "              ])\n",
    "            self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "            return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model = UserModel(use_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model.normalized_timestamp.adapt(\n",
    "    ratings.map(lambda x: x[\"timestamp\"]).batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'138'\n",
      "879024327\n",
      "Computed representations: [-0.02008412  0.04478449 -0.04165016]\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.batch(1).take(1):\n",
    "    print(row['user_id'].numpy()[0])\n",
    "    print(row['timestamp'].numpy()[0])\n",
    "    print(f\"Computed representations: {user_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "          movie_title_lookup,\n",
    "          tf.keras.layers.Embedding(movie_title_lookup.vocab_size(), 32)\n",
    "        ])\n",
    "        \n",
    "        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "          self.title_vectorizer,\n",
    "          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          # We average the embedding of individual words to get one embedding vector\n",
    "          # per title.\n",
    "          tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        self.title_vectorizer.adapt(movies)\n",
    "\n",
    "\n",
    "    def call(self, titles):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(titles),\n",
    "            self.title_text_embedding(titles),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00243484 -0.00526751  0.02876684 -0.00157429  0.01605536  0.04530487\n",
      "  -0.03256219 -0.0481589   0.02902817 -0.04365135  0.0059179  -0.02370564\n",
      "  -0.03139018  0.02896781 -0.02369294  0.0463177  -0.04394087 -0.01041824\n",
      "   0.01856836 -0.01508058 -0.03724738  0.01180527 -0.01620079 -0.0234993\n",
      "   0.00927129 -0.04977113  0.00794138  0.01821036 -0.04593598 -0.03037766\n",
      "   0.02262406 -0.00814469 -0.00873667  0.01587838 -0.01149853 -0.00071614\n",
      "   0.01813574  0.00232467  0.00239427  0.00161277  0.00178416 -0.00589302\n",
      "  -0.00654895  0.01150121  0.0048835  -0.0150386  -0.00294239 -0.00124559\n",
      "  -0.01609775 -0.00117175  0.0070271  -0.01696349  0.02014591  0.00943684\n",
      "  -0.01481183  0.01854273 -0.02757332  0.00213743 -0.00013924  0.00952758\n",
      "   0.01832676 -0.00432802 -0.0031046  -0.01905141]], shape=(1, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "movie_model = MovieModel()\n",
    "\n",
    "movie_model.title_text_embedding.layers[0].adapt(\n",
    "    ratings.map(lambda x: x[\"movie_title\"])\n",
    ")\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "    print(movie_model(row['movie_title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model\n",
    "\n",
    "Note that we also need to make sure that the user model and query model output embeddings of compatible size. Because we'll be varying their sizes by adding more features, the easiest way to accomplish this is to use a dense projection layer after each model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "          UserModel(use_timestamps),\n",
    "          tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "          MovieModel(),\n",
    "          tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "        query_embeddings = self.query_model({\n",
    "            \"user_id\": features[\"user_id\"],\n",
    "            \"timestamp\": features[\"timestamp\"],\n",
    "        })\n",
    "        movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
    "\n",
    "        return self.task(query_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 6s 152ms/step - factorized_top_k: 0.0571 - factorized_top_k/top_1_categorical_accuracy: 0.0083 - factorized_top_k/top_5_categorical_accuracy: 0.0177 - factorized_top_k/top_10_categorical_accuracy: 0.0268 - factorized_top_k/top_50_categorical_accuracy: 0.0843 - factorized_top_k/top_100_categorical_accuracy: 0.1487 - loss: 14589.6233 - regularization_loss: 0.0000e+00 - total_loss: 14589.6233\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 6s 157ms/step - factorized_top_k: 0.0764 - factorized_top_k/top_1_categorical_accuracy: 0.0029 - factorized_top_k/top_5_categorical_accuracy: 0.0146 - factorized_top_k/top_10_categorical_accuracy: 0.0279 - factorized_top_k/top_50_categorical_accuracy: 0.1177 - factorized_top_k/top_100_categorical_accuracy: 0.2188 - loss: 14143.8741 - regularization_loss: 0.0000e+00 - total_loss: 14143.8741\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 7s 164ms/step - factorized_top_k: 0.0875 - factorized_top_k/top_1_categorical_accuracy: 0.0023 - factorized_top_k/top_5_categorical_accuracy: 0.0159 - factorized_top_k/top_10_categorical_accuracy: 0.0309 - factorized_top_k/top_50_categorical_accuracy: 0.1380 - factorized_top_k/top_100_categorical_accuracy: 0.2506 - loss: 13952.3263 - regularization_loss: 0.0000e+00 - total_loss: 13952.3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13ce53cd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MovielensModel(use_timestamps=False)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 6s 154ms/step - factorized_top_k: 0.1054 - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0215 - factorized_top_k/top_10_categorical_accuracy: 0.0417 - factorized_top_k/top_50_categorical_accuracy: 0.1695 - factorized_top_k/top_100_categorical_accuracy: 0.2906 - loss: 13733.8641 - regularization_loss: 0.0000e+00 - total_loss: 13733.8641\n",
      "5/5 [==============================] - 1s 235ms/step - factorized_top_k: 0.0682 - factorized_top_k/top_1_categorical_accuracy: 6.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0078 - factorized_top_k/top_10_categorical_accuracy: 0.0179 - factorized_top_k/top_50_categorical_accuracy: 0.1048 - factorized_top_k/top_100_categorical_accuracy: 0.2098 - loss: 31026.9248 - regularization_loss: 0.0000e+00 - total_loss: 31026.9248\n",
      "Top-100 accuracy (train): 0.29.\n",
      "Top-100 accuracy (test): 0.21.\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 6s 160ms/step - factorized_top_k: 0.0570 - factorized_top_k/top_1_categorical_accuracy: 0.0060 - factorized_top_k/top_5_categorical_accuracy: 0.0151 - factorized_top_k/top_10_categorical_accuracy: 0.0244 - factorized_top_k/top_50_categorical_accuracy: 0.0846 - factorized_top_k/top_100_categorical_accuracy: 0.1552 - loss: 14569.9349 - regularization_loss: 0.0000e+00 - total_loss: 14569.9349\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 6s 161ms/step - factorized_top_k: 0.0917 - factorized_top_k/top_1_categorical_accuracy: 0.0033 - factorized_top_k/top_5_categorical_accuracy: 0.0174 - factorized_top_k/top_10_categorical_accuracy: 0.0334 - factorized_top_k/top_50_categorical_accuracy: 0.1447 - factorized_top_k/top_100_categorical_accuracy: 0.2597 - loss: 13945.8984 - regularization_loss: 0.0000e+00 - total_loss: 13945.8984\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 6s 162ms/step - factorized_top_k: 0.1091 - factorized_top_k/top_1_categorical_accuracy: 0.0028 - factorized_top_k/top_5_categorical_accuracy: 0.0199 - factorized_top_k/top_10_categorical_accuracy: 0.0401 - factorized_top_k/top_50_categorical_accuracy: 0.1757 - factorized_top_k/top_100_categorical_accuracy: 0.3067 - loss: 13690.5271 - regularization_loss: 0.0000e+00 - total_loss: 13690.5271\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 6s 160ms/step - factorized_top_k: 0.1421 - factorized_top_k/top_1_categorical_accuracy: 0.0061 - factorized_top_k/top_5_categorical_accuracy: 0.0347 - factorized_top_k/top_10_categorical_accuracy: 0.0636 - factorized_top_k/top_50_categorical_accuracy: 0.2327 - factorized_top_k/top_100_categorical_accuracy: 0.3734 - loss: 13366.9743 - regularization_loss: 0.0000e+00 - total_loss: 13366.9743\n",
      "5/5 [==============================] - 1s 234ms/step - factorized_top_k: 0.0828 - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0097 - factorized_top_k/top_10_categorical_accuracy: 0.0241 - factorized_top_k/top_50_categorical_accuracy: 0.1304 - factorized_top_k/top_100_categorical_accuracy: 0.2487 - loss: 30703.6419 - regularization_loss: 0.0000e+00 - total_loss: 30703.6419\n",
      "Top-100 accuracy (train): 0.37.\n",
      "Top-100 accuracy (test): 0.25.\n"
     ]
    }
   ],
   "source": [
    "model = MovielensModel(use_timestamps=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "model.fit(cached_train, epochs=3)\n",
    "\n",
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender (Python=3.7)",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
